{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c0ae1d-a3c6-4348-b29a-c01108adb5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
      "\n",
      "by Ambrose Bierce\n",
      "\n",
      "I\n",
      "\n",
      "A man stood upon a railroad bridge in northern Alabama, looking down\n",
      "into the swift water twenty feet below.  The man's hands were behind\n",
      "his back, the wrists bound with a cord.  A rope closely encircled his\n",
      "neck.  It was attached to a stout cross-timber above his head and the\n",
      "slack fell to the level of his knees.  Some loose boards laid upon the\n",
      "ties supporting the rails of\n"
     ]
    }
   ],
   "source": [
    "#1. Create a Doc object from the file owlcreek.txt\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the content of the file\n",
    "with open(\"owlcreek.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text to create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Now 'doc' is a spaCy Doc object\n",
    "print(doc[:100])  # Print the first 100 tokens of the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf46dcb-3281-4f1c-beee-b1106a5a8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 4835 tokens.\n"
     ]
    }
   ],
   "source": [
    "#2. How many tokens are contained in the file?\n",
    "\n",
    "num_tokens = len(doc)\n",
    "print(f\"The file contains {num_tokens} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30bde84-4ee9-46f1-8e73-ac2f3be0578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file contains 204 sentences.\n"
     ]
    }
   ],
   "source": [
    "#3. How many sentences are contained in the file?\n",
    "num_sentences = len(list(doc.sents))\n",
    "print(f\"The file contains {num_sentences} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd5d4e5-6c73-447a-a1f8-2710821f2bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second sentence is: The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n"
     ]
    }
   ],
   "source": [
    "#4. Print the second sentence in the document\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Check if there are at least two sentences\n",
    "if len(sentences) >= 2:\n",
    "    print(f\"The second sentence is: {sentences[1].text}\")\n",
    "else:\n",
    "    print(\"There is no second sentence in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc75d9a-11a2-4c2b-a6c7-7f69ef2f5d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The, POS: DET, Dep: det, Lemma: the\n",
      "Text: man, POS: NOUN, Dep: poss, Lemma: man\n",
      "Text: 's, POS: PART, Dep: case, Lemma: 's\n",
      "Text: hands, POS: NOUN, Dep: nsubj, Lemma: hand\n",
      "Text: were, POS: AUX, Dep: ROOT, Lemma: be\n",
      "Text: behind, POS: ADP, Dep: prep, Lemma: behind\n",
      "Text: \n",
      ", POS: SPACE, Dep: dep, Lemma: \n",
      "\n",
      "Text: his, POS: PRON, Dep: poss, Lemma: his\n",
      "Text: back, POS: NOUN, Dep: pobj, Lemma: back\n",
      "Text: ,, POS: PUNCT, Dep: punct, Lemma: ,\n",
      "Text: the, POS: DET, Dep: det, Lemma: the\n",
      "Text: wrists, POS: NOUN, Dep: appos, Lemma: wrist\n",
      "Text: bound, POS: VERB, Dep: acl, Lemma: bind\n",
      "Text: with, POS: ADP, Dep: prep, Lemma: with\n",
      "Text: a, POS: DET, Dep: det, Lemma: a\n",
      "Text: cord, POS: NOUN, Dep: pobj, Lemma: cord\n",
      "Text: ., POS: PUNCT, Dep: punct, Lemma: .\n",
      "Text:  , POS: SPACE, Dep: dep, Lemma:  \n"
     ]
    }
   ],
   "source": [
    "#5. For each token in the sentence above, print its text, POS tag, dep tag and lemma.\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "# Check if there are at least two sentences\n",
    "if len(sentences) >= 2:\n",
    "    second_sentence = sentences[1]\n",
    "\n",
    "    # Print the token details for each token in the second sentence\n",
    "    for token in second_sentence:\n",
    "        print(f\"Text: {token.text}, POS: {token.pos_}, Dep: {token.dep_}, Lemma: {token.lemma_}\")\n",
    "else:\n",
    "    print(\"There is no second sentence in the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb101f-8930-48ca-9cd8-7a69488b0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the content of the file\n",
    "with open(\"owlcreek.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text to create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern for \"swimming vigorously\"\n",
    "pattern = [\n",
    "    {\"lower\": \"swimming\"},  # Token \"swimming\" (case-insensitive)\n",
    "    {\"lower\": \"vigorously\"}  # Token \"vigorously\" (case-insensitive)\n",
    "]\n",
    "\n",
    "# Add the pattern to the matcher with the label \"Swimming\"\n",
    "matcher.add(\"Swimming\", [pattern])\n",
    "\n",
    "# Find matches in the document\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print the matches\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]  # Get the span of the matched phrase\n",
    "    print(f\"Match '{span.text}' found at position {start}-{end}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
